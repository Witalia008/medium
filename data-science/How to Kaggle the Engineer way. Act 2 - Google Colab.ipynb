{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae97367c",
   "metadata": {},
   "source": [
    "# How to Kaggle the Engineer way. Act 2: Google Colab\n",
    "\n",
    "Or, how to engineer the heck out of your Kaggle development environment: a play in 2 acts.\n",
    "\n",
    "This is the Act 2 of the play, and you can read Act 1 [here](https://link.medium.com/kOoq47KYrfb).\n",
    "\n",
    "My entire Kaggle work, including what's discussed in this article, is published [here](https://github.com/Witalia008/kaggle-public)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316e677",
   "metadata": {},
   "source": [
    "## Prologue/Intro\n",
    "\n",
    "In the first part of the story I described how to set up VS Code with Containers for Kaggle. To me it is of the utmost importance to use methodical approaches, should it be engineering work, or data science. Because of that belief, my ramp up on Kaggle competitions was in large part setting things up so that I would save time in the future.\n",
    "\n",
    "In this article, I'll describe how I improved on my initial approach of using VS Code for development for the second competition I took part in. As the title suggests, I moved to Google Colab, but nonetheless I still wanted to have proper set up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b98119",
   "metadata": {},
   "source": [
    "## Shortcomings of previous approach\n",
    "\n",
    "In the first competition, having set up VS Code with Kaggle container, I was developing locally and training on Kaggle. I did my best to minimize any repetitive tasks involved in switching between the environments. However, there were still a few other issues:\n",
    "* I still had to switch between two environments.\n",
    "* Kaggle would not allow to use specific version of the dataset (for instance, when training 5 fold, I'd have to download each and upload all as a dataset for the ensemble).\n",
    "* Including dependencies was a pain - I had to upload them, again, as a dataset and then map their location.\n",
    "\n",
    "In this article I'll describe how I used Google Colab to overcome all of these nuisances and get some additional perks like better GPU and better runtime in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee76817",
   "metadata": {},
   "source": [
    "## Act 2 - Google Colaboratory for Kaggle with GitHub\n",
    "\n",
    "Google Colab is a great way to get free GPU for your Data Science work. For a small additional fee, you could get Colab pro with V100/P100 GPU, which is a few times faster than T4 ones that you get on Kaggle. And it has other nice features, like mapping Google Drive files, working with GitHub, and more - described below.\n",
    "\n",
    "I chose that it would be an improvement over Kaggle in training speed and ease of use, and I would not need to develop locally. With a bit more research, it looked like I won't even need to leave Colab environment and do everything within (kind of like not leaving my flat)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325452d",
   "metadata": {},
   "source": [
    "### Setting up GitHub for Colab in Google Drive\n",
    "\n",
    "First of the things I did was to make sure I can still work with Git, and would not have to switch between the environments.\n",
    "\n",
    "The bad approach: you can open files directly from your private or public repo, edit it and commit back, as described [here](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb). There are a few issues, however:\n",
    "* the commits are only for one notebook, i.e. Colab only works within the context of one notebook.\n",
    "* because of the above, you cannot have dependencies.\n",
    "\n",
    "The good thing is, Colab allows you to [mount your Google Drive](https://www.marktechpost.com/2019/06/07/how-to-connect-google-colab-with-google-drive/).\n",
    "Once it gets mapped, it is a regular folder on your VM instance in which Colab is running.\n",
    "\n",
    "So, why not clone your GitHub repo?\n",
    "* Create GitHub Personal Access Token to use in place of your password - [link](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token).\n",
    "* Have some temporary notebook, let's call it `terminal.ipynb`, stored in your Google Drive, and with your drive mapped:\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "```\n",
    "* In Colab Pro, you can then open a terminal instance - [link](https://stackoverflow.com/questions/59318692/how-can-i-run-shell-terminal-in-google-colab).\n",
    "* In any Colab, you can run terminal commands from the notebook cells:\n",
    "```python\n",
    "%%bash\n",
    "cd /content/drive/MyDrive/\n",
    "git clone https://<username>:<PAT>@github.com/<username>/<repo name>.git\n",
    "cd <repo name>\n",
    "```\n",
    "\n",
    "At this point, you have all your code git-cloned to your Google Drive, and you can open any notebook within.\n",
    "I've also set up some code in a [notebook](https://github.com/Witalia008/kaggle-public/blob/master/setup-colab-for-kaggle.ipynb) to work with Git, but I ended up using just the terminal commands.\n",
    "\n",
    "The only shortcoming is that you'd have to map Google Drive at the beginning of each notebook, but I think that's relatively fine.\n",
    "```python\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    %cd /content/drive/MyDrive/Colab\\ Notebooks/kaggle\n",
    "except:\n",
    "    print(\"Not in Colab\")\n",
    "```\n",
    "I've created myself a [template notebook](https://github.com/Witalia008/kaggle-public/blob/master/template-colab-kaggle-nb.ipynb) which I copy as a baseline for any other notebook.\n",
    "\n",
    "NOTE: an additional cool hack, not to have to give Colab authorization to your Drive each time, you can set up auto-mount for each of the notebooks like described [here](https://stackoverflow.com/questions/52808143/colab-automatic-authentication-of-connection-to-google-drive-persistent-per-n)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c7a34",
   "metadata": {},
   "source": [
    "### Set up for Kaggle\n",
    "\n",
    "Next step was to set up all the directories needed for Kaggle (`/kaggle/input`, `/kaggle/working`) and Kaggle API secrets, etc. This setup would have to be done at the beginning of each notebook, so it would have to be as script imported and functions called.\n",
    "\n",
    "You cannot really edit Python scripts in Colab, but you can make your notebooks write to files:\n",
    "```python\n",
    "%%writefile setup_colab.py\n",
    "def setup_colab_for_kaggle():\n",
    "    ...\n",
    "```\n",
    "\n",
    "In that script, let's perform the necessary setup:\n",
    "* Map and/or create directories:\n",
    "```python\n",
    "kaggle_dir = Path(\"/kaggle\")\n",
    "drive_content_dir = Path(\"/content/drive/MyDrive/kaggle\")\n",
    "(kaggle_dir / \"working\").mkdir()\n",
    "target_content_dirs = [\"input\", \"output\"] + ([] if local_working else [\"working\"])\n",
    "for content_dir in target_content_dirs:\n",
    "    (kaggle_dir / content_dir).symlink_to(drive_content_dir / content_dir)\n",
    "```\n",
    "* Set up Kaggle API token. Upload your `kaggle.json` file to your Drive (I placed it in the repo folder, though of you do too, **make sure to add it to the `.gitignore`**).\n",
    "```python\n",
    "drive_sources_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/kaggle\")\n",
    "kaggle_config = Path.home() / \".kaggle\"\n",
    "(kaggle_config / \"kaggle.json\").symlink_to(drive_sources_dir / \"kaggle.json\")\n",
    "```\n",
    "* You could use the same approach to set up Weights & Biases API key or any other key.\n",
    "* Have any other set up code in that file.\n",
    "\n",
    "There's quite a bit of additional logic, so the full code is in the [notebook](https://github.com/Witalia008/kaggle-public/blob/master/setup-colab-for-kaggle.ipynb) and the [output script](https://github.com/Witalia008/kaggle-public/blob/master/setup_colab.py).\n",
    "\n",
    "Now, each notebook would just have to import the code and run the setup function:\n",
    "```python\n",
    "from setup_colab import setup_colab_for_kaggle\n",
    "setup_colab_for_kaggle(check_env=False, local_working=True)\n",
    "```\n",
    "\n",
    "Using my script, the output would tell us about the resulting setup:\n",
    "```\n",
    "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
    "/content/drive/MyDrive/Colab Notebooks/kaggle\n",
    "Content of Drive Kaggle data dir (/content/drive/MyDrive/kaggle): ['/content/drive/MyDrive/kaggle/input', '/content/drive/MyDrive/kaggle/working', '/content/drive/MyDrive/kaggle/output']\n",
    "Content of Kaggle data dir (/kaggle): ['/kaggle/working', '/kaggle/output', '/kaggle/input']\n",
    "Content of Kaggle data subdir (/kaggle/input): ['/kaggle/input/vinbigdata']\n",
    "Content of Kaggle data subdir (/kaggle/output): ['/kaggle/output/vbdyolo-out']\n",
    "Content of Kaggle data subdir (/kaggle/working): []\n",
    "Content of Kaggle config dir (/root/.kaggle): ['/root/.kaggle/kaggle.json']\n",
    "Loaded environment variables from .env file: ['WANDB_API_KEY'].\n",
    "```\n",
    "\n",
    "Cool, after this point, all of the setup is mimicking that of Kaggle kernels, and of the local environment described in previous article, so the notebooks wouldn't know the difference when switching between the two environments (should one want to). But let's set this up even more so we won't have to switch at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf724e8c",
   "metadata": {},
   "source": [
    "### Getting data from Kaggle\n",
    "\n",
    "At first, I would download the data locally, then upload it to my Drive, and it would end up being mapped to the correct location as shown above. However, that wasn't too practical, and most importantly - *Colab was very slow in accessing files from Google Drive*. Turns out, it's much faster to download the dataset each time into local storage of Colab's VM. In your notebook just have:\n",
    "```\n",
    "!kaggle competitions download vinbigdata-chest-xray-abnormalities-detection -f train.csv -p {INPUT_FOLDER_DATA} --unzip\n",
    "!kaggle datasets download xhlulu/vinbigdata-chest-xray-resized-png-1024x1024 -p {INPUT_FOLDER_PNG} --unzip\n",
    "```\n",
    "\n",
    "One epoch of training Efficient Net B3 through Google Drive would take ~1 hour, whereas downloading 8GB takes around 8 minutes and training 1 epoch comes down to ~15 minutes. 4x speed improvement - not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb172466",
   "metadata": {},
   "source": [
    "### Versioning datasets and models\n",
    "\n",
    "This section will shed some light on why we needed to set up Kaggle API token.\n",
    "\n",
    "Being systematic and methodical about my approaches to Kaggle competitions, I started thinking how to organize the data I use and the models that get produced. Kind of like Git for models and data. And, in fact, I found out there's [Data Version Control](https://dvc.org/), and some other tools. However, just for Kaggle competitions, something simpler would do. One option would be just to store in folders, but that's not quite neat and data could be lost, and it's hard to attach notes to each version. So I chose to stole everything as Kaggle datasets.\n",
    "\n",
    "That could be done as easily as uploading files at the end of each notebook:\n",
    "* Place the files you want to upload in one folder (let's say, `yolo_pred/` folder with YOLO predictions and `yolo.pt` file).\n",
    "* Create `dataset-metadata.json` file - something parsed and understood by Kaggle API:\n",
    "```python\n",
    "with open(Path(folder_path) / \"dataset-metadata.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"title\": dataset_name,\n",
    "        \"id\": f\"{user_name}/{dataset_name}\",\n",
    "        \"licenses\": [{ \"name\": \"CC0-1.0\" }]\n",
    "    }, f, indent=4)\n",
    "```\n",
    "* Use Kaggle API command to upload the files at the end of your notebook execution (e.g. model training):\n",
    "If first time:\n",
    "```\n",
    "!kaggle datasets create -p {OUTPUT_FOLDER_CUR} -r zip\n",
    "```\n",
    "OR if not:\n",
    "```\n",
    "!kaggle datasets version -m \"{version_message}\" -p {OUTPUT_FOLDER_CUR} -r zip\n",
    "```\n",
    "* Now, in your post-processing, you can download a specific version (specific trained mode and its output) to use for submission:\n",
    "```\n",
    "!kaggle datasets download \"username/dataset-name\" -v {yolo_version} -p {version_data[\"path\"]} --unzip --force\n",
    "```\n",
    "Spoiler alert: though, this won't work in official Kaggle API - see next paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b054d",
   "metadata": {},
   "source": [
    "### Fixing Kaggle API for our needs\n",
    "\n",
    "As a bit of digression from the main story of setting everything up I needed to add/fix a few features in Kaggle API. The push for this was because at one point I wasn't able to reproduce good result I obtained with some earlier version. At that point I had everything versioned, and every setting recorded, so that was close to impossible... Yet it was happening. And then I noticed I was getting the same result for any of the versions I was selecting. Turned out, well, that Kaggle API was silently ignoring my version request and always downloading the latest one.\n",
    "\n",
    "So, their GitHub repo mentioned they welcome changes, hence I decided to take matters into my own hands (I quite needed the functionality for the competition).\n",
    "* First, I needed to make sure Kaggle would download a specific version. My PR [here](https://github.com/Kaggle/kaggle-api/pull/335).\n",
    "* Second, I was already on the roll, so I made some fixes that one file from the competition would download correctly (the full dataset was ~200GB, and I was downloading just the `.csv` file with train labels, and getting ~8GB of the same images from another dataset). My PR [here](https://github.com/Kaggle/kaggle-api/pull/336).\n",
    "\n",
    "Apparently, though, the process of getting changes into Kaggle API is slow (they have a private repo, and some dev has to go and merge the changes...).\n",
    "So, both changes are temporarily in my fork [here](https://github.com/Witalia008/kaggle-api/tree/witalia-main).\n",
    "\n",
    "You can install Kaggle API with my fixes from your Colab notebook like so:\n",
    "```\n",
    "!pip install -U git+https://github.com/Witalia008/kaggle-api.git@witalia-main\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef35db",
   "metadata": {},
   "source": [
    "### Submitting to Kaggle\n",
    "\n",
    "For the particular competition I was taking part in, the submission was just a `.csv` file, so I would simply run the following command at the end of my post-processing notebook:\n",
    "```bash\n",
    "!kaggle competitions submit \\\n",
    "    vinbigdata-chest-xray-abnormalities-detection \\\n",
    "    -f {WORK_FOLDER}/submission.csv \\\n",
    "    -m \"{submission_message}\"\n",
    "```\n",
    "\n",
    "And then check on the result of that submission:\n",
    "```bash\n",
    "!kaggle competitions submissions vinbigdata-chest-xray-abnormalities-detection\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eefe55",
   "metadata": {},
   "source": [
    "### Entire setup\n",
    "\n",
    "You can refer to the files with the complete implementation:\n",
    "* [setup-colab-for-kaggle.ipynb](https://github.com/Witalia008/kaggle-public/blob/master/setup-colab-for-kaggle.ipynb) - the notebook with code to work with Git, and also setup-colab logic.\n",
    "* [setup_colab.py](https://github.com/Witalia008/kaggle-public/blob/master/setup_colab.py) - the setup script (as produced by the above notebook), which contains functions to map directories, configure keys, etc.\n",
    "* [template-colab-kaggle-nb.ipynb](https://github.com/Witalia008/kaggle-public/blob/master/template-colab-kaggle-nb.ipynb) - the template notebook that has required setup which I use as a base for other notebooks.\n",
    "* [vbd-yolov5.ipynb](https://github.com/Witalia008/kaggle-public/blob/master/vinbigdata-chest-xray-abnormalities-detection/vbd-yolov5.ipynb) - example notebook that downloads datasets, trains a model, and stores output as a dataset.\n",
    "* [vbd-postprocess-yolo.ipynb](https://github.com/Witalia008/kaggle-public/blob/master/vinbigdata-chest-xray-abnormalities-detection/vbd-postprocess-yolo.ipynb) - example notebook that downloads multiple versions of the previous notebook's outputs as datasets, ensembles them, and submits to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f11d79",
   "metadata": {},
   "source": [
    "### Further improvements\n",
    "\n",
    "The problem with this approach is actually losing the full IDE's capabilities (like, debugging, or working with other file formats apart from notebooks). There's limited debugging capabilities with pdb in notebooks, and one can edit files with Vim or Nano through the terminal, but that's not very good. But I hear there's a way to connect VS Code to Google Colab ([link](https://amitness.com/vscode-on-colab/)), so I'll definitely try that out and maybe there will be a sequel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3afab",
   "metadata": {},
   "source": [
    "## Epilogue/Summary\n",
    "\n",
    "The setup described in this article is further improvement for Kaggle development environment, that with a few simple steps allows to download data, do your data science work, store the models/output, and submit to competitions, without ever leaving Colab notebooks.\n",
    "\n",
    "This could be achieved by the following steps:\n",
    "* Mounting Google Drive and operating through terminal commands.\n",
    "* Cloning Git repo and tracking files as you would do on local machine.\n",
    "* Creating a script to run setup commands at the beginning of each notebook to mimic Kaggle environment, setup API keys, and more.\n",
    "* Using Kaggle API to download datasets and competition data from notebooks into Colab VM.\n",
    "* Using Kaggle API to store model versions and their outputs as datasets.\n",
    "* Using Kaggle API to submit to competitions directly from notebooks.\n",
    "\n",
    "I hope that you find this setup useful and it allows you to spend less time on configuring everything and more time on solving for Kaggle competitions. Good luck :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafeadf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
